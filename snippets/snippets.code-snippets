{
    "MapReduce starter template": {
        "prefix": "!starter",
        "body": [
            "import org.apache.hadoop.io.IntWritable;",
            "import org.apache.hadoop.io.LongWritable;",
            "import org.apache.hadoop.io.Text;",
            "import org.apache.hadoop.mapreduce.Mapper;",
            "import org.apache.hadoop.mapreduce.Reducer;",
            "import org.apache.hadoop.mapreduce.Job;",
            "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;",
            "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;",
            "import org.apache.hadoop.fs.Path;",
            "import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;",
            "import org.apache.hadoop.conf.Configuration;",
            "import java.io.IOException;",
            "$0",
            "public class Example {",
            "$0",
            "    public static class ExampleMapper ",
            "        extends Mapper<Text, Text, Text, Text > {",
            "$0",
            "        @Override",
            "        public void map(Text key, Text value, Context context)",
            "            throws IOException, InterruptedException {",
            "$0",
            "            context.write(key, value);",
            "$0",
            "        }",
            "    }",
            "$0",
            "    public static class ExampleReducer",
            "        extends  Reducer<Text, Text, Text, Text> {",
            "$0",
            "        @Override",
            "        public void reduce(Text key, Iterable<Text> values, Context context)",
            "            throws IOException, InterruptedException {",
            "$0",
            "            String content = \"\";",
            "$0",
            "            for (Text val : values) {",
            "                content = val.toString();",
            "            }",
            "$0",
            "            context.write(key, new Text(content));",
            "        }",
            "    }",
            "$0",
            "    public static void main(String[] args) throws Exception {",
            "        Configuration conf = new Configuration();",
            "        conf.set(\"mapreduce.input.keyvaluelinerecordreader.key.value.separator\", \",\");",
            "$0",
            "        Job  job = Job.getInstance(conf);",
            "$0",
            "        job.setJarByClass(Example.class);  ",
            "$0",
            "        KeyValueTextInputFormat.addInputPath(job, new Path(args[0]));",
            "        FileOutputFormat.setOutputPath(job, new Path(args[1]));",
            "        job.setInputFormatClass(KeyValueTextInputFormat.class);",
            "$0",
            "        job.setMapperClass(ExampleMapper.class);",
            "        job.setReducerClass(ExampleReducer.class);",
            "$0",
            "        job.setOutputKeyClass(Text.class);",
            "        job.setOutputValueClass(Text.class);",
            "$0",
            "        job.setJobName(\"Job name\");",
            "$0",
            "        System.exit(job.waitForCompletion(true) ? 0:1);",
            "    }",
            "}"
        ],
        "description": "Creates basic KeyValueTextInputFormat template"
    },
    "Chained MapReduce Jobs (word count)": {
        "prefix": "!wc",
        "body": [
            "import org.apache.hadoop.io.IntWritable;",
            "import org.apache.hadoop.io.LongWritable; ",
            "import org.apache.hadoop.io.Text; ",
            "import org.apache.hadoop.mapreduce.Mapper;",
            "import org.apache.hadoop.mapreduce.Reducer;",
            "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;",
            "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;",
            "import org.apache.hadoop.fs.Path;",
            "import java.io.IOException;",
            "$0",
            "public class Filter {",
            "    public static class counterMapper     ",
            "        extends Mapper< LongWritable, Text, Text, LongWritable > {",
            "        @Override",
            "        public void map(LongWritable key, Text value, Context context)",
            "            throws IOException, InterruptedException {",
            "            context.write(value, new LongWritable(1));",
            "        }",
            "    }",
            "$0",
            "    public static class counterReducer  ",
            "        extends  Reducer< Text, LongWritable, Text, LongWritable> {",
            "        @Override  ",
            "        public void reduce( Text key, Iterable<LongWritable> values, Context context)",
            "            throws IOException, InterruptedException {",
            "            long sum = 0; ",
            "            for(LongWritable one : values) {",
            "            sum = sum+ one.get();",
            "            } ",
            "            context.write(key, new LongWritable(sum));",
            "            }",
            "        }",
            "$0",
            "    public static class totalCountMapper  ",
            "        extends Mapper< LongWritable, Text, LongWritable, LongWritable > {",
            "        @Override",
            "        public void map(LongWritable key, Text value, Context context)",
            "            throws IOException, InterruptedException {",
            "            context.write(new LongWritable(1), new LongWritable(1));",
            "        }",
            "    }",
            "$0",
            "    public static class totalCountReducer ",
            "        extends  Reducer< LongWritable, LongWritable, LongWritable, LongWritable> {",
            "        @Override",
            "        public void reduce( LongWritable key, Iterable<LongWritable> values, Context context)",
            "            throws IOException, InterruptedException {",
            "            long sum = 0; ",
            "            for(LongWritable one : values) {",
            "                sum = sum+ one.get();",
            "            }",
            "            context.write(key, new LongWritable(sum));",
            "        }",
            "    }",
            "$0",
            "public static void main(String[] args) throws Exception {",
            "    Job  job = Job.getInstance(); ",
            "    job.setJarByClass(Filter.class);  ",
            "    FileInputFormat.addInputPath(job, new Path(args[0]));",
            "    FileOutputFormat.setOutputPath(job, new Path(args[1]));",
            "    job.setMapperClass(counterMapper.class);",
            "    job.setReducerClass(counterReducer.class);",
            "    job.setOutputKeyClass(Text.class); ",
            "    job.setOutputValueClass(LongWritable.class);",
            "    job.setJobName(\"Chains\");",
            "    job.waitForCompletion(true);",
            "    Job countAllJob = Job.getInstance();",
            "    countAllJob.setJarByClass(filter.class);",
            "    FileInputFormat.addInputPath(countAllJob, new Path(args[2]));",
            "    FileOutputFormat.setOutputPath(countAllJob, new Path(args[3])); ",
            "    countAllJob.setMapperClass(totalCountMapper.class);",
            "    countAllJob.setReducerClass(totalCountReducer.class);",
            "    countAllJob.setOutputKeyClass(LongWritable.class);",
            "    countAllJob.setOutputValueClass(LongWritable.class);",
            "    countAllJob.setJobName(\"to count\");",
            "    System.exit(countAllJob.waitForCompletion(true) ? 0: 1);",
            "  }",
            "}",
        ],
        "description": "Create chained mapReducer example as word count"
    }
}